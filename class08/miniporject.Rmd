---
title: "Mini project1"
author: "Norman Lee"
date: "2025-10-29"
output: html_document
---


**Background**
The goal of this mini project is to explore a complete analysis using the unsupervised machine learning technique covered in our class.
The data itself comes from the wisconsin breast cancer diagnotic data set first reported by KP Benne and OL Mangasarian 

**Data import**
```{r}
wisc.df <- read.csv("C:/Users/leeho/OneDrive/桌面/bimm 143/lab8/WisconsinCancer.csv", row.names = 1)
head(wisc.df)
```

**Data eploration**
The first comlumn diagnoisis) is the expert opinion on the sample
```{r}
head(wisc.df$diagnosis)
```
```{r}
head(wisc.df[,-1])
```
remove the diagnosis from the data fro subsequent analysis

```{r}
wisc.data <- wisc.df[,-1]
dim(wisc.data)
```
Store the diagnosis as a vector for later use when we copare our result to those from expoert in the field

```{r}
diagnosis <- factor(wisc.df$diagnosis)
```

>Q1. How many observations are in this dataset?
there are 569 observations

>Q2. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```
>Q3. How many variables/features in the data are suffixed with _mean

```{r}
colnames(wisc.data)
```
```{r}
length(grep("_mean",colnames(wisc.data)))
```
**PCA**
The next step in your analysis is to perform principal component analysis (PCA) on wisc.data.

It is important to check if the data need to be scaled before performing PCA. Recall two common reasons for scaling data include:

The input variables use different units of measurement.
The input variables have significantly different variances.
Check the mean and standard deviation of the features (i.e. columns) of the wisc.data to determine if the data should be scaled. Use the colMeans() and apply() functions like you’ve done before.
```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```


      Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
PC1= 0.4427
      Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
pc1=0.4427, pc2=0.6324, pc3=0.72636
      Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
pc6=0.88759, pc7=0.91010
7 pc are required to explain at least 90% of the variance

**PCA score plot **

```{r}
library(ggplot2)

ggplot(wisc.pr$x)+
  aes(PC1,PC2, col=diagnosis)+
  geom_point()
```

>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

The separation between groups is lesss distanct. Most of the variationis stilll along pc1, while pc3 adds new information and doesn't separate into cluster

**PCA screeplot**

```{r}
wisc.pr$sdev
```
```{r}
var.tbl <- summary(wisc.pr)
head(var.tbl$importance)
```
```{r}
var <- var.tbl$importance[2,]
cum.var <- var.tbl$importance[3,]

plot(var*100, type="b")
  ylab="Percent Variance Captured"
  xlab="PC number"
```

plot
```{r}
plot(cum.var*100, type="o",
  ylab="Percent Variance Captured",
  xlab="PC number")
ylim=c(0,100)
points(var*100, col='blue',type="o")
```

communicating pca results 

>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation["concave.points_mean",1]
```
>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

we need 5 pcs to capture more than 80% variance
```{r}
summary(wisc.pr)
```
**Hierchical culustrering**

```{r}
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist)
plot(wisc.hclust)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
table(wisc.hclust.clusters)
```


PCA and clustering 
```{r}
dist.pc <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(dist.pc, method="ward.D2")

plot(wisc.pr.hclust)
abline(h=70, col="red")
```
>Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
grps <- cutree(wisc.pr.hclust, h=70)
table(grps)

```
Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

The best alighment with the diagnose was when the tree was cut to yield two clusters 

Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.
ward d2 gave a cleaner and distinct clusters







